{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb14783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njfernandez/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, DatasetDict, ClassLabel, concatenate_datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#Seeding for deterministic results i.e. showing same output \n",
    "RANDOM_SEED = 64\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "   torch.cuda.manual_seed(RANDOM_SEED)\n",
    "   torch.cuda.manual_seed_all(RANDOM_SEED) \n",
    "   torch.backends.cudnn.deterministic = True  \n",
    "   torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d72be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-051610f9ffe8f8d2\n",
      "Found cached dataset csv (/home/njfernandez/.cache/huggingface/datasets/csv/default-051610f9ffe8f8d2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 521.10it/s]\n",
      "Using custom data configuration default-94907eb33ba58000\n",
      "Found cached dataset csv (/home/njfernandez/.cache/huggingface/datasets/csv/default-94907eb33ba58000/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 762.18it/s]\n",
      "Using custom data configuration default-869e6abb15784ff2\n",
      "Found cached dataset csv (/home/njfernandez/.cache/huggingface/datasets/csv/default-869e6abb15784ff2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 816.07it/s]\n",
      "Loading cached shuffled indices for dataset at /home/njfernandez/.cache/huggingface/datasets/csv/default-051610f9ffe8f8d2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-342257832c8283f6.arrow\n",
      "Loading cached shuffled indices for dataset at /home/njfernandez/.cache/huggingface/datasets/csv/default-051610f9ffe8f8d2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-61b64f8f2d33fef0.arrow\n",
      "Loading cached shuffled indices for dataset at /home/njfernandez/.cache/huggingface/datasets/csv/default-051610f9ffe8f8d2/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-22704bf1ca817405.arrow\n"
     ]
    }
   ],
   "source": [
    "function_mapping = {'OTHER': ['anticipation', 'joy', 'love', 'optimism', 'surprise', 'trust'],\n",
    "           'NOT_INTERESTED': [''], \n",
    "           'DISLIKE':['disgust'], \n",
    "           'NOT_CORRECT': [''], \n",
    "           'PESSIMISTIC':['sadness', 'pessimism'], \n",
    "           'WORRIED':['fear'], \n",
    "           'ANGRY': ['anger'], \n",
    "           'DISAPPOINTED': [''], \n",
    "           'BORED': [''], \n",
    "           'NOT_APPROVE':[''], \n",
    "           'NOT_IMPORTANT': [''], \n",
    "           'DISAGREE': [''], \n",
    "           'WARN': [''], \n",
    "           'COMPLAIN': [''], \n",
    "           'THREATEN': [''], \n",
    "           'UNWILLING': [''], \n",
    "           'DISTRUST' : [''],\n",
    "           'REFUSE': [''] }\n",
    "\n",
    "ait_es_dataset = load_dataset(\"csv\", sep = '\\t', data_files ={'train': r'2018-E-c-Es-train.txt',\n",
    "                                                        'test': r'2018-E-c-Es-test-gold.txt',\n",
    "                                                       'valid': r'2018-E-c-Es-dev.txt'})\n",
    "ait_en_dataset = load_dataset(\"csv\", sep = '\\t', data_files ={'train': r'2018-E-c-En-train.txt',\n",
    "                                                        'test': r'2018-E-c-En-test-gold.txt',\n",
    "                                                       'valid': r'2018-E-c-En-dev.txt'})\n",
    "ait_ar_dataset = load_dataset(\"csv\", sep = '\\t', data_files ={'train': r'2018-E-c-Ar-train.txt',\n",
    "                                                        'test': r'2018-E-c-Ar-test-gold.txt',\n",
    "                                                       'valid': r'2018-E-c-Ar-dev.txt'})\n",
    "train_dataset = concatenate_datasets([ait_es_dataset['train'], ait_en_dataset['train'], ait_ar_dataset['train']])\n",
    "valid_dataset = concatenate_datasets([ait_es_dataset['valid'], ait_en_dataset['valid'], ait_ar_dataset['valid']])\n",
    "test_dataset = concatenate_datasets([ait_es_dataset['test'], ait_en_dataset['test'], ait_ar_dataset['test']])\n",
    "\n",
    "ait_dataset = DatasetDict({'train': train_dataset.shuffle(seed=42), 'valid': valid_dataset.shuffle(seed=42), 'test': test_dataset.shuffle(seed=42)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe70f991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '2018-Ar-00161',\n",
       " 'Tweet': '...\\\\nÿßŸÑÿ≥ŸÉŸàÿ™ ÿπŸÖÿ±Ÿá ŸÖÿß ŸÉÿßŸÜ ÿπŸÑÿßŸÖÿ© ÿ±ÿ∂ÿß ÿßŸÑÿ≥ŸÉŸàÿ™ ŸÜŸÅÿßÿ∞ ÿµÿ®ÿ± ŸàŸàÿ¨ÿπ ÿ®ÿ≥ ÿßŸÜÿ™Ÿà ÿßŸÑŸÑŸä ŸÖÿ®ÿ™ÿ≠ÿ≥Ÿàÿ¥...üíî\\\\n\\\\n....',\n",
       " 'anger': 1,\n",
       " 'anticipation': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 0,\n",
       " 'joy': 0,\n",
       " 'love': 0,\n",
       " 'optimism': 0,\n",
       " 'pessimism': 0,\n",
       " 'sadness': 1,\n",
       " 'surprise': 0,\n",
       " 'trust': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ait_dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5999de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = DatasetDict()\n",
    "for split in ait_dataset:\n",
    "    new_split = []\n",
    "    for record in ait_dataset[split]:\n",
    "        new_record = {'Tweet': record['Tweet']}\n",
    "        for function in function_mapping:\n",
    "            labels = function_mapping[function]\n",
    "            if '' in labels:\n",
    "                continue\n",
    "            else:\n",
    "                score = sum([record[label] for label in labels])\n",
    "                new_record[function] = int(score > 0)\n",
    "        new_split.append(new_record)\n",
    "    ait_dataset[split] =  Dataset.from_pandas(pd.DataFrame(data=new_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a817835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tweet': '...\\\\nÿßŸÑÿ≥ŸÉŸàÿ™ ÿπŸÖÿ±Ÿá ŸÖÿß ŸÉÿßŸÜ ÿπŸÑÿßŸÖÿ© ÿ±ÿ∂ÿß ÿßŸÑÿ≥ŸÉŸàÿ™ ŸÜŸÅÿßÿ∞ ÿµÿ®ÿ± ŸàŸàÿ¨ÿπ ÿ®ÿ≥ ÿßŸÜÿ™Ÿà ÿßŸÑŸÑŸä ŸÖÿ®ÿ™ÿ≠ÿ≥Ÿàÿ¥...üíî\\\\n\\\\n....',\n",
       " 'OTHER': 0,\n",
       " 'DISLIKE': 1,\n",
       " 'PESSIMISTIC': 1,\n",
       " 'WORRIED': 0,\n",
       " 'ANGRY': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ait_dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1320904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Tweet', 'OTHER', 'DISLIKE', 'PESSIMISTIC', 'WORRIED', 'ANGRY'],\n",
       "        num_rows: 12675\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['Tweet', 'OTHER', 'DISLIKE', 'PESSIMISTIC', 'WORRIED', 'ANGRY'],\n",
       "        num_rows: 2150\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Tweet', 'OTHER', 'DISLIKE', 'PESSIMISTIC', 'WORRIED', 'ANGRY'],\n",
       "        num_rows: 7631\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ait_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8492272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12675/12675 [00:00<00:00, 13398.33ex/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2150/2150 [00:00<00:00, 13499.49ex/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7631/7631 [00:00<00:00, 11472.19ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tweet': '...\\\\nÿßŸÑÿ≥ŸÉŸàÿ™ ÿπŸÖÿ±Ÿá ŸÖÿß ŸÉÿßŸÜ ÿπŸÑÿßŸÖÿ© ÿ±ÿ∂ÿß ÿßŸÑÿ≥ŸÉŸàÿ™ ŸÜŸÅÿßÿ∞ ÿµÿ®ÿ± ŸàŸàÿ¨ÿπ ÿ®ÿ≥ ÿßŸÜÿ™Ÿà ÿßŸÑŸÑŸä ŸÖÿ®ÿ™ÿ≠ÿ≥Ÿàÿ¥...üíî\\\\n\\\\n....',\n",
       " 'OTHER': 0,\n",
       " 'DISLIKE': 1,\n",
       " 'PESSIMISTIC': 1,\n",
       " 'WORRIED': 0,\n",
       " 'ANGRY': 1,\n",
       " 'functions': ['DISLIKE', 'PESSIMISTIC', 'ANGRY']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ait_dataset['train'].column_names\n",
    "ait_dataset = ait_dataset.map(lambda x : {\"functions\": [c for c in cols if x[c] == 1]})\n",
    "ait_dataset['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77eeec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ait_dataset.keys():\n",
    "    new_examples = []\n",
    "    for example in ait_dataset[split]:\n",
    "        for label in example['functions']:\n",
    "            new_example = {'text': example['Tweet'], 'function': label}\n",
    "            new_examples.append(new_example)\n",
    "    ait_dataset[split] = Dataset.from_pandas(pd.DataFrame(data=new_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1991dbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'function'],\n",
       "        num_rows: 20918\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'function'],\n",
       "        num_rows: 3448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'function'],\n",
       "        num_rows: 12172\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ait_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c5717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to class labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:00<00:00, 538.15ba/s]\n",
      "Casting to class labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 534.60ba/s]\n",
      "Casting to class labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 580.61ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANGRY', 'DISLIKE', 'OTHER', 'PESSIMISTIC', 'WORRIED']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = ait_dataset.class_encode_column(\"function\")\n",
    "labels = dataset['train'].features['function']\n",
    "dataset = dataset.rename_column(\"function\", \"label\")\n",
    "print(labels.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c15c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.67s/ba]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.81ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 20918\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 3448\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 12172\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "transformer_model = 'cardiffnlp/twitter-xlm-roberta-base'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(transformer_model, num_labels = len(labels.names))).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(transformer_model)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "ds_enc = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\")\n",
    "ds_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e77a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/njfernandez/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20918\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6540\n",
      "  Number of trainable parameters = 278047493\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/njfernandez/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6012' max='6540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6012/6540 37:54 < 03:19, 2.64 it/s, Epoch 4.60/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.266800</td>\n",
       "      <td>1.137835</td>\n",
       "      <td>0.506869</td>\n",
       "      <td>0.461188</td>\n",
       "      <td>0.425374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.159100</td>\n",
       "      <td>1.087745</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0.499993</td>\n",
       "      <td>0.471894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.065600</td>\n",
       "      <td>1.076084</td>\n",
       "      <td>0.497321</td>\n",
       "      <td>0.502257</td>\n",
       "      <td>0.475068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>1.093479</td>\n",
       "      <td>0.489892</td>\n",
       "      <td>0.501097</td>\n",
       "      <td>0.478936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3448\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to baseline_finetuning_single_label/checkpoint-1308\n",
      "Configuration saved in baseline_finetuning_single_label/checkpoint-1308/config.json\n",
      "Model weights saved in baseline_finetuning_single_label/checkpoint-1308/pytorch_model.bin\n",
      "tokenizer config file saved in baseline_finetuning_single_label/checkpoint-1308/tokenizer_config.json\n",
      "Special tokens file saved in baseline_finetuning_single_label/checkpoint-1308/special_tokens_map.json\n",
      "/home/njfernandez/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3448\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to baseline_finetuning_single_label/checkpoint-2616\n",
      "Configuration saved in baseline_finetuning_single_label/checkpoint-2616/config.json\n",
      "Model weights saved in baseline_finetuning_single_label/checkpoint-2616/pytorch_model.bin\n",
      "tokenizer config file saved in baseline_finetuning_single_label/checkpoint-2616/tokenizer_config.json\n",
      "Special tokens file saved in baseline_finetuning_single_label/checkpoint-2616/special_tokens_map.json\n",
      "Deleting older checkpoint [baseline_finetuning_single_label/checkpoint-1308] due to args.save_total_limit\n",
      "/home/njfernandez/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3448\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to baseline_finetuning_single_label/checkpoint-3924\n",
      "Configuration saved in baseline_finetuning_single_label/checkpoint-3924/config.json\n",
      "Model weights saved in baseline_finetuning_single_label/checkpoint-3924/pytorch_model.bin\n",
      "tokenizer config file saved in baseline_finetuning_single_label/checkpoint-3924/tokenizer_config.json\n",
      "Special tokens file saved in baseline_finetuning_single_label/checkpoint-3924/special_tokens_map.json\n",
      "Deleting older checkpoint [baseline_finetuning_single_label/checkpoint-2616] due to args.save_total_limit\n",
      "/home/njfernandez/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3448\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to baseline_finetuning_single_label/checkpoint-5232\n",
      "Configuration saved in baseline_finetuning_single_label/checkpoint-5232/config.json\n",
      "Model weights saved in baseline_finetuning_single_label/checkpoint-5232/pytorch_model.bin\n",
      "tokenizer config file saved in baseline_finetuning_single_label/checkpoint-5232/tokenizer_config.json\n",
      "Special tokens file saved in baseline_finetuning_single_label/checkpoint-5232/special_tokens_map.json\n",
      "Deleting older checkpoint [baseline_finetuning_single_label/checkpoint-3924] due to args.save_total_limit\n",
      "/home/njfernandez/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 8\n",
    "model_name = \"baseline_finetuning_single_label\"\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate = 2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model = \"f1\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit = 1,        \n",
    "        report_to='none',\n",
    "    )\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=ds_enc[\"train\"],\n",
    "                  eval_dataset=ds_enc[\"valid\"],                    \n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=data_collator,)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35341334",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(ds_enc['test'])\n",
    "print(preds_output.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.array(ds_enc['test'][\"label\"])\n",
    "y_pred = preds_output.predictions.argmax(-1)\n",
    "\n",
    "report = classification_report(\n",
    "  y_true,\n",
    "  y_pred,\n",
    "  target_names=labels.names,\n",
    "  zero_division=0\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(\n",
    "  y_true,\n",
    "  y_pred,\n",
    "  target_names=labels.names,\n",
    "  zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "with open(r'classification_report_baseline_ait_finetuning_singlelabel.csv', 'w') as csv_file:\n",
    "    df.to_csv(path_or_buf=csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66941a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):    \n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(50, 50))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)  \n",
    "    font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 16}\n",
    "    plt.rc('font', **font)\n",
    "    ax.tick_params(axis='x', which='major', labelsize=15)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b409bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_valid = np.array(ds_enc[\"test\"][\"label\"])\n",
    "plot_confusion_matrix(y_preds, y_valid, labels.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n",
    "                             reduction=\"none\")     \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}\n",
    "\n",
    "ds_enc.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "ds_enc[\"test\"] = ds_enc[\"test\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return dataset[\"train\"].features[\"label\"].int2str(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee99dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_enc.set_format(\"pandas\")\n",
    "cols = [\"text\", 'label', \"predicted_label\", \"loss\"]\n",
    "df_test = ds_enc[\"test\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                              .apply(label_int2str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = df_test.sort_values(\"loss\", ascending=False).head(50)\n",
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values.to_csv('loss_values_baseline_ait_finetuning_singlelabel.tsv',header =True, sep = '\\t',index=False)\n",
    "df_test.to_csv('preds_baseline_ait_singlelabel.tsv',header =True, sep = '\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284a045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
